{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一章、知识库文档处理\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本项目是一个 `个人知识库助手项目` ，旨在帮助用户根据个人知识库内容，回答用户问题。个人知识库应当能够支持各种类型的数据，支持用户便捷地导入导出、进行管理。在我们的项目中，我们以 Datawhale 的一些经典开源课程作为示例，设计了多种文件类型，介绍每一种文件类型的处理方式，从而支持用户无难度地构建自己的知识库。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、知识库设计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的知识库选用 Datawhale 一些经典开源课程、视频（部分）作为示例，具体包括：\n",
    "\n",
    "· [《机器学习公式详解》PDF版本](https://github.com/datawhalechina/pumpkin-book/releases)\n",
    "\n",
    "· [《面向开发者的 LLM 入门教程`第一部分 Prompt Engineering》md版本](https://github.com/datawhalechina/prompt-engineering-for-developers)\n",
    "\n",
    "· [《强化学习入门指南》MP4版本](https://www.bilibili.com/video/BV1HZ4y1v7eX/?spm_id_from=333.999.0.0&vd_source=4922e78f7a24c5981f1ddb6a8ee55ab9)\n",
    "\n",
    "我们会将知识库源数据放置在 ../../data_base/knowledge_db 目录下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、 文档加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. PDF 文档"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用 PyMuPDFLoader 来读取知识库的 PDF 文件。PyMuPDFLoader 是 PDF 解析器中速度最快的一种，结果会包含 PDF 及其页面的详细元数据，并且每页返回一个文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 安装必要的库\n",
    "# !pip install rapidocr_onnxruntime -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip install \"unstructured[all-docs]\" -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip install pyMuPDF -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# 创建一个 PyMuPDFLoader Class 实例，输入为待加载的 pdf 文档路径\n",
    "loader = PyMuPDFLoader(\"../../data_base/knowledge_db/pumkin_book/pumpkin_book.pdf\")\n",
    "\n",
    "# 调用 PyMuPDFLoader Class 的函数 load 对 pdf 文件进行加载\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 探索加载的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文档加载后储存在 `pages` 变量中:\n",
    "- `page` 的变量类型为 `List`\n",
    "- 打印 `pages` 的长度可以看到 pdf 一共包含多少页"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "载入后的变量类型为：<class 'list'>， 该 PDF 一共包含 196 页\n"
     ]
    }
   ],
   "source": [
    "print(f\"载入后的变量类型为：{type(pages)}，\",  f\"该 PDF 一共包含 {len(pages)} 页\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`page` 中的每一元素为一个文档，变量类型为 `langchain.schema.document.Document`, 文档变量类型包含两个属性\n",
    "- `page_content` 包含该文档的内容。\n",
    "- `meta_data` 为文档相关的描述性数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每一个元素的类型：<class 'langchain.schema.document.Document'>.\n",
      "------\n",
      "该文档的描述性数据：{'source': '../../data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'file_path': '../../data_base/knowledge_db/pumkin_book/pumpkin_book.pdf', 'page': 1, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230303170709-00'00'\", 'modDate': '', 'trapped': ''}\n",
      "------\n",
      "查看该文档的内容:\n",
      "前言\n",
      "“周志华老师的《机器学习》（西瓜书）是机器学习领域的经典入门教材之一，周老师为了使尽可能多的读\n",
      "者通过西瓜书对机器学习有所了解, 所以在书中对部分公式的推导细节没有详述，但是这对那些想深究公式推\n",
      "导细节的读者来说可能“不太友好”，本书旨在对西瓜书里比较难理解的公式加以解析，以及对部分公式补充\n",
      "具体的推导细节。”\n",
      "读到这里，大家可能会疑问为啥前面这段话加了引号，因为这只是我们最初的遐想，后来我们了解到，周\n",
      "老师之所以省去这些推导细节的真实原因是，他本尊认为“理工科数学基础扎实点的大二下学生应该对西瓜书\n",
      "中的推导细节无困难吧，要点在书里都有了，略去的细节应能脑补或做练习”。所以...... 本南瓜书只能算是我\n",
      "等数学渣渣在自学的时候记下来的笔记，希望能够帮助大家都成为一名合格的“理工科数学基础扎实点的大二\n",
      "下学生”。\n",
      "使用说明\n",
      "• 南瓜书的所有内容都是以西瓜书的内容为前置知识进行表述的，所以南瓜书的最佳使用方法是以西瓜书\n",
      "为主线，遇到自己推导不出来或者看不懂的公式时再来查阅南瓜书；\n",
      "• 对于初学机器学习的小白，西瓜书第 1 章和第 2 章的公式强烈不建议深究，简单过一下即可，等你学得\n",
      "有点飘的时候再回来啃都来得及；\n",
      "• 每个公式的解析和推导我们都力 (zhi) 争 (neng) 以本科数学基础的视角进行讲解，所以超纲的数学知识\n",
      "我们通常都会以附录和参考文献的形式给出，感兴趣的同学可以继续沿着我们给的资料进行深入学习；\n",
      "• 若南瓜书里没有你想要查阅的公式，或者你发现南瓜书哪个地方有错误，请毫不犹豫地去我们 GitHub 的\n",
      "Issues（地址：https://github.com/datawhalechina/pumpkin-book/issues）进行反馈，在对应版块\n",
      "提交你希望补充的公式编号或者勘误信息，我们通常会在 24 小时以内给您回复，超过 24 小时未回复的\n",
      "话可以微信联系我们（微信号：at-Sm1les）；\n",
      "配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\n",
      "在线阅读地址：https://datawhalechina.github.io/pumpkin-book（仅供第 1 版）\n",
      "最新版 PDF 获取地址：https://github.com/datawhalechina/pumpkin-book/re\n"
     ]
    }
   ],
   "source": [
    "page = pages[1]\n",
    "print(f\"每一个元素的类型：{type(page)}.\", \n",
    "    f\"该文档的描述性数据：{page.metadata}\", \n",
    "    f\"查看该文档的内容:\\n{page.page_content[0:1000]}\", \n",
    "    sep=\"\\n------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. MD 文档"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以以几乎完全一致的方式读入 markdown 文档："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "loader = UnstructuredMarkdownLoader(\"../../data_base/knowledge_db/prompt_engineering/1. 简介 Introduction.md\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取的对象和 PDF 文档读取出来是完全一致的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "载入后的变量类型为：<class 'list'>， 该 Markdown 一共包含 1 页\n"
     ]
    }
   ],
   "source": [
    "print(f\"载入后的变量类型为：{type(pages)}，\",  f\"该 Markdown 一共包含 {len(pages)} 页\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每一个元素的类型：<class 'langchain.schema.document.Document'>.\n",
      "------\n",
      "该文档的描述性数据：{'source': '../../data_base/knowledge_db/prompt_engineering/1. 简介 Introduction.md'}\n",
      "------\n",
      "查看该文档的内容:\n",
      "第一章 简介\n",
      "\n",
      "欢迎来到面向开发者的提示工程部分，本部分内容基于吴恩达老师的《Prompt Engineering for Developer》课程进行编写。《Prompt Engineering for Developer》课程是由吴恩达老师与 OpenAI 技术团队成员 Isa Fulford 老师合作授课，Isa 老师曾开发过受欢迎的 ChatGPT 检索插件，并且在教授 LLM （Large Language Model， 大语言模型）技术在产品中的应用方面做出了很大贡献。她还参与编写了教授人们使用 Prompt 的 OpenAI cookbook。我们希望通过本模块的学习，与大家分享使用提示词开发 LLM 应用的最佳实践和技巧。\n",
      "\n",
      "网络上有许多关于提示词（Prompt， 本教程中将保留该术语）设计的材料，例如《30 prompts everyone has to know》之类的文章，这些文章主要集中在 ChatGPT 的 Web 界面上，许多人在使用它执行特定的、通常是一次性的任务。但我们认为，对于开发人员，大语言模型（LLM） 的更强大功能是能通过 API 接口调用，从而快速构建软件应用程序。实际上，我们了解到 DeepLearning.AI 的姊妹公司 AI Fund 的团队一直在与许多初创公司合作，将这些技术应用于诸多应用程序上。很兴奋能看到 LLM API 能够让开发人员非常快速地构建应用程序。\n",
      "\n",
      "在本模块，我们将与读者分享提升大语言模型应用效果的各种技巧和最佳实践。书中内容涵盖广泛，包括软件开发提示词设计、文本总结、推理、转换、扩展以及构建聊天机器人等语言模型典型应用场景。我们衷心希望该课程能激发读者的想象力，开发出更出色的语言模型应用。\n",
      "\n",
      "随着 LLM 的发展，其大致可以分为两种类型，后续称为基础 LLM 和指令微调（Instruction Tuned）LLM。基础LLM是基于文本训练数据，训练出预测下一个单词能力的模型。其通常通过在互联网和其他来源的大量数据上训练，来确定紧接着出现的最可能的词。例如，如果你以“从前，有一只独角兽”作为 Prompt ，基础 LLM 可能会继续预测“她与独角兽朋友共同生活在一片神奇森林中”。但是，如果你以“法国的首都是什么”为 Prompt ，则基础 LLM 可能会根据互联网上的文章，将回答预测为“法国最大的城市是什么？法国的人口是多少？”，因为互联网上的文章很可能是有关法国国家的问答题目列表。\n",
      "\n",
      "与基础语言模型不同，指令微调 LLM 通过专门的训练，可以更好地理解并遵循指令。举个例子，当询问“法国的首都是什么？”时，这类模型很可能直接回答“法国的首都是巴黎”。指令微调 LLM 的训练通常基于预训练语言模型，先在大规模文本数据上进行预训练，掌握语言的基本规律。在此基础上进行进一步的训练与微调（finetune），输入是指令，输出是对这些指令的正确回复。有时还会采用RLHF（reinforcement learning from human feedback，人类反馈强化学习）技术，根据人类对模型输出的反馈进一步增强模型遵循指令的能力。通过这种受控的训练过程。指令微调 LLM 可以生成对指令高度敏感、更安全可靠的输出，较少无关和损害性内容。因此。许多实际应用已经转向使用这类大语言模型。\n",
      "\n",
      "因此，本课程将重点介绍针对指令微调 LLM 的最佳实践，我们也建议您将其用于大多数使用场景。当您使用指令微调 LLM 时，您可以类比为向另一个人提供指令（假设他很聪明但不知道您任务的具体细节）。因此，当 LLM 无法正常工作时，有时是因为指令不够清晰。例如，如果您想问“请为我写一些关于阿兰·图灵( Alan Turing )的东西”，在此基础上清楚表明您希望文本专注于他的科学工作、个人生活、历史角色或其他方面可能会更有帮助。另外您还可以指定回答的语调， 来更加满足您的需求，可选项包括专业记者写作，或者向朋友写的随笔等。\n",
      "\n",
      "如果你将 LLM 视为一名新毕业的大学生，要求他完成这个任务，你甚至可以提前指定他们应该阅读哪些文本片段来写关于阿兰·图灵的文本，这样能够帮助这位新毕业的大学生更好地完成这项任务。本书的下一章将详细阐释提示词设计的两个关键原则：清晰明确和给予充足思考时间。\n"
     ]
    }
   ],
   "source": [
    "page = pages[0]\n",
    "print(f\"每一个元素的类型：{type(page)}.\", \n",
    "    f\"该文档的描述性数据：{page.metadata}\", \n",
    "    f\"查看该文档的内容:\\n{page.page_content[0:]}\", \n",
    "    sep=\"\\n------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. MP4 视频"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain 提供了对 Youtube 视频进行爬取并转写的处理接口，但是如果我们想直接对我们的本地 MP4 视频进行处理，需要首先经过转录加载成文本格式，在加载到 LangChain 中。\n",
    "\n",
    "我们使用 Whisper 实现视频的转写，Whisper 的安装方式此处不再赘述，详见教程：[知乎|开源免费离线语音识别神器whisper如何安装](https://zhuanlan.zhihu.com/p/595691785)\n",
    "\n",
    "此处我们直接使用 Whisper 在原目录下输出转写结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "whisper ../../data_base/knowledge_db/easy_rl/强化学习入门指南.mp4 --model large --model_dir whisper-large --language zh --output_dir ../../data_base/knowledge_db/easy_rl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，此处 model_dir 参数应是你下载到本地的 large-whisper 参数路径。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转化完后，会在原目录下生成 强化学习入门指南.txt 文件，我们直接加载该 txt 文件即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "loader = UnstructuredFileLoader(\"../../data_base/knowledge_db/easy_rl/强化学习入门指南.txt\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载出来的数据属性同上文一致："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每一个元素的类型：<class 'langchain.schema.document.Document'>.\n",
      "------\n",
      "该文档的描述性数据：{'source': '../../data_base/knowledge_db/easy_rl/强化学习入门指南.txt'}\n",
      "------\n",
      "查看该文档的内容:\n",
      "B站的小伙伴们好\n",
      "\n",
      "我是蘑菇书一语二语二强化学习教程的作者之一王奇\n",
      "\n",
      "今天来有给大家带来一个强化学习的入门指南\n",
      "\n",
      "本次入门指南基于蘑菇书一语二语二强化学习教程\n",
      "\n",
      "本书的作者目前都是Dell会员成员\n",
      "\n",
      "也都是数学在读\n",
      "\n",
      "下面去介绍每个作者\n",
      "\n",
      "我是王奇\n",
      "\n",
      "目前就留于中国科研院大学\n",
      "\n",
      "引用方向是深度学习、静态视觉以及数据挖掘\n",
      "\n",
      "杨玉云目前就读于清华大学\n",
      "\n",
      "他的引用方向为\n",
      "\n",
      "时空数据挖掘、智能冲砍系统以及深度学习\n",
      "\n",
      "张记目前就读于北京大学\n",
      "\n",
      "他的引用方向为强化学习记忆人\n",
      "\n",
      "接下来开始正式的分享\n",
      "\n",
      "本次分享分为三部分\n",
      "\n",
      "第一部分\n",
      "\n",
      "为什么要学强化学习\n",
      "\n",
      "第二部分\n",
      "\n",
      "为什么要用本书来学\n",
      "\n",
      "第三部分\n",
      "\n",
      "这本书怎么学最高效\n",
      "\n",
      "首先讲一下为什么要学强化学习\n",
      "\n",
      "我们先聊一下强化学习的基本概念\n",
      "\n",
      "强化学习用来学习如何做出一系列好的决策\n",
      "\n",
      "而人工智能的基本挑战是\n",
      "\n",
      "学习在不确定的情况下做出好的决策\n",
      "\n",
      "这边我举个例子\n",
      "\n",
      "比如你想让一个小孩学会走路\n",
      "\n",
      "他就需要通过不断尝试来发现\n",
      "\n",
      "怎么走比较好\n",
      "\n",
      "怎么走比较快\n",
      "\n",
      "强化学习的交互过程可以通过这张图来表示\n",
      "\n",
      "强化学习由智能体和环境两部分组成\n",
      "\n",
      "在强化学习过程中\n",
      "\n",
      "智能体与环境一直在交互\n",
      "\n",
      "智能体在环境中获取某个状态后\n",
      "\n",
      "它会利用刚刚的状态输出一个动作\n",
      "\n",
      "这个动作也被称为决策\n",
      "\n",
      "然后这个动作会被环境中执行\n",
      "\n",
      "环境会根据智能体采取的动作\n",
      "\n",
      "来输出下一个状态\n",
      "\n",
      "以及当前这个动作带来的奖励\n",
      "\n",
      "整体它的目的呢\n",
      "\n",
      "就是尽可能的多的\n",
      "\n",
      "尽可能多的在环境中获取奖励\n",
      "\n",
      "强化学习的应用非常广泛\n",
      "\n",
      "比如说我们可以使用强化学习来玩游戏\n",
      "\n",
      "玩游戏的话可以玩这种电子游戏\n",
      "\n",
      "也可以玩这种围棋游戏\n",
      "\n",
      "围棋游戏中比较出名的一个\n",
      "\n",
      "强化学习的算法就是AlphaGo\n",
      "\n",
      "此外我们可以使用强化学习\n",
      "\n",
      "来控制机器人\n",
      "\n",
      "以及来实现助力交通\n",
      "\n",
      "另外还可以使用强化学习\n",
      "\n",
      "来更好地给我们做推进\n",
      "\n",
      "接下来就到第二部分\n",
      "\n",
      "也就是为什么要使用本书来学习强化学习\n",
      "\n",
      "这部分其实也是讲\n",
      "\n",
      "这个蘑菇书它出版的一些故事\n",
      "\n",
      "当时我在学习强化学习的时候\n",
      "\n",
      "搜集了一些资料\n",
      "\n",
      "然后我发现这些资料\n",
      "\n",
      "都有点灰色难懂\n",
      "\n",
      "并不是那么容易地上手\n",
      "\n",
      "于是我开始到网上\n",
      "\n",
      "搜索一些公开课来学习\n",
      "\n",
      "首先我搜索到的是\n",
      "\n",
      "李宏毅老师的一些公开课\n",
      "\n",
      "很多人就是入门深度学习\n",
      "\n",
      "和基学低门课\n",
      "\n",
      "其实就是李宏毅老师的课\n",
      "\n",
      "李宏毅老师的课\n",
      "\n",
      "李宏毅老师\n"
     ]
    }
   ],
   "source": [
    "page = pages[0]\n",
    "print(f\"每一个元素的类型：{type(page)}.\", \n",
    "    f\"该文档的描述性数据：{page.metadata}\", \n",
    "    f\"查看该文档的内容:\\n{page.page_content[0:1000]}\", \n",
    "    sep=\"\\n------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、文档分割\n",
    "Langchain 中文本分割器都根据 `chunk_size` (块大小)和 `chunk_overlap` (块与块之间的重叠大小)进行分割。\n",
    "\n",
    "![image.png](../../figures/example-splitter.png)\n",
    "\n",
    "* chunk_size 指每个块包含的字符或 Token （如单词、句子等）的数量\n",
    "\n",
    "* chunk_overlap 指两个块之间共享的字符数量，用于保持上下文的连贯性，避免分割丢失上下文信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain 提供多种文档分割方式，区别在怎么确定块与块之间的边界、块由哪些字符/token组成、以及如何测量块大小\n",
    "\n",
    "- RecursiveCharacterTextSplitter(): 按字符串分割文本，递归地尝试按不同的分隔符进行分割文本。\n",
    "- CharacterTextSplitter(): 按字符来分割文本。\n",
    "- MarkdownHeaderTextSplitter(): 基于指定的标题来分割markdown 文件。\n",
    "- TokenTextSplitter(): 按token来分割文本。\n",
    "- SentenceTransformersTokenTextSplitter(): 按token来分割文本\n",
    "- Language(): 用于 CPP、Python、Ruby、Markdown 等。\n",
    "- NLTKTextSplitter(): 使用 NLTK（自然语言工具包）按句子分割文本。\n",
    "- SpacyTextSplitter(): 使用 Spacy按句子的切割文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "* RecursiveCharacterTextSplitter 递归字符文本分割\n",
    "RecursiveCharacterTextSplitter 将按不同的字符递归地分割(按照这个优先级[\"\\n\\n\", \"\\n\", \" \", \"\"])，\n",
    "    这样就能尽量把所有和语义相关的内容尽可能长时间地保留在同一位置\n",
    "RecursiveCharacterTextSplitter需要关注的是4个参数：\n",
    "\n",
    "* separators - 分隔符字符串数组\n",
    "* chunk_size - 每个文档的字符数量限制\n",
    "* chunk_overlap - 两份文档重叠区域的长度\n",
    "* length_function - 长度计算函数\n",
    "'''\n",
    "#导入文本分割器\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 知识库中单段文本长度\n",
    "CHUNK_SIZE = 500\n",
    "\n",
    "# 知识库中相邻文本重合长度\n",
    "OVERLAP_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['前言\\n“周志华老师的《机器学习》（西瓜书）是机器学习领域的经典入门教材之一，周老师为了使尽可能多的读\\n者通过西瓜书对机器学习有所了解, 所以在书中对部分公式的推导细节没有详述，但是这对那些想深究公式推\\n导细节的读者来说可能“不太友好”，本书旨在对西瓜书里比较难理解的公式加以解析，以及对部分公式补充\\n具体的推导细节。”\\n读到这里，大家可能会疑问为啥前面这段话加了引号，因为这只是我们最初的遐想，后来我们了解到，周\\n老师之所以省去这些推导细节的真实原因是，他本尊认为“理工科数学基础扎实点的大二下学生应该对西瓜书\\n中的推导细节无困难吧，要点在书里都有了，略去的细节应能脑补或做练习”。所以...... 本南瓜书只能算是我\\n等数学渣渣在自学的时候记下来的笔记，希望能够帮助大家都成为一名合格的“理工科数学基础扎实点的大二\\n下学生”。\\n使用说明\\n• 南瓜书的所有内容都是以西瓜书的内容为前置知识进行表述的，所以南瓜书的最佳使用方法是以西瓜书\\n为主线，遇到自己推导不出来或者看不懂的公式时再来查阅南瓜书；\\n• 对于初学机器学习的小白，西瓜书第 1 章和第 2 章的公式强烈不建议深究，简单过一下即可，等你学得',\n",
       " '有点飘的时候再回来啃都来得及；\\n• 每个公式的解析和推导我们都力 (zhi) 争 (neng) 以本科数学基础的视角进行讲解，所以超纲的数学知识\\n我们通常都会以附录和参考文献的形式给出，感兴趣的同学可以继续沿着我们给的资料进行深入学习；\\n• 若南瓜书里没有你想要查阅的公式，或者你发现南瓜书哪个地方有错误，请毫不犹豫地去我们 GitHub 的\\nIssues（地址：https://github.com/datawhalechina/pumpkin-book/issues）进行反馈，在对应版块\\n提交你希望补充的公式编号或者勘误信息，我们通常会在 24 小时以内给您回复，超过 24 小时未回复的\\n话可以微信联系我们（微信号：at-Sm1les）；\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n在线阅读地址：https://datawhalechina.github.io/pumpkin-book（仅供第 1 版）\\n最新版 PDF 获取地址：https://github.com/datawhalechina/pumpkin-book/re']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 此处我们使用 PDF 文件作为示例\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# 创建一个 PyMuPDFLoader Class 实例，输入为待加载的 pdf 文档路径\n",
    "loader = PyMuPDFLoader(\"../../data_base/knowledge_db/pumkin_book/pumpkin_book.pdf\")\n",
    "\n",
    "# 调用 PyMuPDFLoader Class 的函数 load 对 pdf 文件进行加载\n",
    "pages = loader.load()\n",
    "page = pages[1]\n",
    "\n",
    "# 使用递归字符文本分割器\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=OVERLAP_SIZE\n",
    ")\n",
    "text_splitter.split_text(page.page_content[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "切分后的文件数量：737\n"
     ]
    }
   ],
   "source": [
    "split_docs = text_splitter.split_documents(pages)\n",
    "print(f\"切分后的文件数量：{len(split_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "切分后的字符数（可以用来大致评估 token 数）：314712\n"
     ]
    }
   ],
   "source": [
    "print(f\"切分后的字符数（可以用来大致评估 token 数）：{sum([len(doc.page_content) for doc in split_docs])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、文档词向量化\n",
    "\n",
    "在机器学习和自然语言处理（NLP）中，`Embeddings`（嵌入）是一种将类别数据，如单词、句子或者整个文档，转化为实数向量的技术。这些实数向量可以被计算机更好地理解和处理。嵌入背后的主要想法是，相似或相关的对象在嵌入空间中的距离应该很近。\n",
    "\n",
    "举个例子，我们可以使用词嵌入（word embeddings）来表示文本数据。在词嵌入中，每个单词被转换为一个向量，这个向量捕获了这个单词的语义信息。例如，\"king\" 和 \"queen\" 这两个单词在嵌入空间中的位置将会非常接近，因为它们的含义相似。而 \"apple\" 和 \"orange\" 也会很接近，因为它们都是水果。而 \"king\" 和 \"apple\" 这两个单词在嵌入空间中的距离就会比较远，因为它们的含义不同。\n",
    "\n",
    "让我们取出我们的切分部分并对它们进行 `Embedding` 处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里提供三种方式进行，一种是直接使用 openai 的模型去生成 embedding，另一种是使用 HuggingFace 上的模型去生成 embedding。\n",
    "\n",
    "- openAI 的模型需要消耗 api，对于大量的token 来说成本会比较高，但是非常方便。\n",
    "- HuggingFace 的模型可以本地部署，可自定义合适的模型，可玩性较高，但对本地的资源有部分要求。\n",
    "- 采用其他平台的 api。对于获取 openAI key 不方便的同学可以采用这种方法。\n",
    "\n",
    "对于只想体验一下的同学来说，可以尝试直接用生成好的 embedding，或者在本地部署小模型进行尝试。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HuggingFace 是一个优秀的开源库，我们只需要输入模型的名字，就会自动帮我们解析对应的能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用前配置自己的 api 到环境变量中如\n",
    "import os\n",
    "import openai\n",
    "import zhipuai\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # read local .env fileopenai.api_key  = os.environ['OPENAI_API_KEY']\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']\n",
    "zhipuai.api_key = os.environ['ZHIPUAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from zhipuai_embedding import ZhipuAIEmbeddings\n",
    "\n",
    "# embedding = OpenAIEmbeddings() \n",
    "# embedding = HuggingFaceEmbeddings(model_name=\"moka-ai/m3e-base\")\n",
    "embedding = ZhipuAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"机器学习\"\n",
    "query2 = \"强化学习\"\n",
    "query3 = \"大语言模型\"\n",
    "\n",
    "# 通过对应的 embedding 类生成 query 的 embedding。\n",
    "emb1 = embedding.embed_query(query1)\n",
    "emb2 = embedding.embed_query(query2)\n",
    "emb3 = embedding.embed_query(query3)\n",
    "\n",
    "# 将返回结果转成 numpy 的格式，便于后续计算\n",
    "emb1 = np.array(emb1)\n",
    "emb2 = np.array(emb2)\n",
    "emb3 = np.array(emb3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以直接查看 embedding 的具体信息，embedding 的维度通常取决于所使用的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "机器学习 生成的为长度 1024 的 embedding , 其前 30 个值为： [-0.02768379  0.07836673  0.1429528  -0.1584693   0.08204    -0.15819356\n",
      " -0.01282174  0.18076552  0.20916627  0.21330206 -0.1205181  -0.06666514\n",
      " -0.16731478  0.31798768  0.0680017  -0.13807729 -0.03469152  0.15737721\n",
      "  0.02108428 -0.29145902 -0.10099868  0.20487919 -0.03603597 -0.09646764\n",
      "  0.12923686 -0.20558454  0.17238656  0.03429411  0.1497675  -0.25297147]\n"
     ]
    }
   ],
   "source": [
    "print(f\"{query1} 生成的为长度 {len(emb1)} 的 embedding , 其前 30 个值为： {emb1[:30]}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们已经生成了对应的向量，我们如何度量文档和问题的相关性呢？\n",
    "\n",
    "这里提供两种常用的方法：\n",
    "- 计算两个向量之间的点积。\n",
    "- 计算两个向量之间的余弦相似度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "点积是将两个向量对应位置的元素相乘后求和得到的标量值。点积相似度越大，表示两个向量越相似。\n",
    "\n",
    "这里直接使用 numpy 的函数进行计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "机器学习 和 强化学习 向量之间的点积为：17.218882120572722\n",
      "机器学习 和 大语言模型 向量之间的点积为：16.522186236712727\n",
      "强化学习 和 大语言模型 向量之间的点积为：11.368461841901752\n"
     ]
    }
   ],
   "source": [
    "print(f\"{query1} 和 {query2} 向量之间的点积为：{np.dot(emb1, emb2)}\")\n",
    "print(f\"{query1} 和 {query3} 向量之间的点积为：{np.dot(emb1, emb3)}\")\n",
    "print(f\"{query2} 和 {query3} 向量之间的点积为：{np.dot(emb2, emb3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "点积：计算简单，快速，不需要进行额外的归一化步骤，但丢失了方向信息。\n",
    "\n",
    "余弦相似度：可以同时比较向量的方向和数量级大小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "余弦相似度将两个向量的点积除以它们的模长的乘积。其基本的计算公式为 $$cos(\\theta) = {\\sum_{i=1}^{n}{(x_i \\times y_i)} \\over {\\sum_{i=1}^{n}{(x_i)^2} \\times \\sum_{i=1}^{n}{(y_i)^2}}}$$余弦函数的值域在-1到1之间，即两个向量余弦相似度的范围是[-1, 1]。当两个向量夹角为0°时，即两个向量重合时，相似度为1；当夹角为180°时，即两个向量方向相反时，相似度为-1。即越接近于 1 越相似，越接近 0 越不相似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "机器学习 和 强化学习 向量之间的余弦相似度为：[[0.68814796]]\n",
      "机器学习 和 大语言模型 向量之间的余弦相似度为：[[0.63382724]]\n",
      "强化学习 和 大语言模型 向量之间的余弦相似度为：[[0.43555894]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"{query1} 和 {query2} 向量之间的余弦相似度为：{cosine_similarity(emb1.reshape(1, -1) , emb2.reshape(1, -1) )}\")\n",
    "print(f\"{query1} 和 {query3} 向量之间的余弦相似度为：{cosine_similarity(emb1.reshape(1, -1) , emb3.reshape(1, -1) )}\")\n",
    "print(f\"{query2} 和 {query3} 向量之间的余弦相似度为：{cosine_similarity(emb2.reshape(1, -1) , emb3.reshape(1, -1) )}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，模型认为机器学习和强化学习更相关一点，强化学习和大语言模型之间的相关性更差。（这部分跟训练语料的时间相关，embedding 的模型应该没有大语言模型相关的语料。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前，我们已经学习了文档的基本处理，但是如何管理我们生成的 embedding 并寻找和 query 最相关的内容呢？难道要每次遍历所有文档么？向量数据库可以帮我们快速的管理和计算这些内容。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
