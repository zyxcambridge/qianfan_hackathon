{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 第二章、向量数据库的介绍及使用\n","\n"," - [一、向量数据库简介](#一、向量数据库简介)\n"," - [二、构建 Chroma 向量库](#二、构建-Chroma-向量库)\n"," - [三、通过向量数据库检索](#三、通过向量数据库检索)\n","     - [3.1 相似度检索](#3.1-相似度检索)\n","     - [3.2 MMR 检索](#3.2-MMR-检索)\n"," - [四、构造检索式问答连](#四、构造检索式问答连)\n","     - [4.1 直接询问 LLM](#4.1-直接询问-LLM)\n","     - [4.2 结合 prompt 提问](#4.2-结合-prompt-提问)\n"]},{"cell_type":"markdown","metadata":{},"source":["## 一、向量数据库简介"]},{"cell_type":"markdown","metadata":{},"source":["向量数据库是用于高效计算和管理大量向量数据的解决方案。向量数据库是一种专门用于存储和检索向量数据（embedding）的数据库系统。它与传统的基于关系模型的数据库不同，它主要关注的是向量数据的特性和相似性。\n","\n","在向量数据库中，数据被表示为向量形式，每个向量代表一个数据项。这些向量可以是数字、文本、图像或其他类型的数据。向量数据库使用高效的索引和查询算法来加速向量数据的存储和检索过程。"]},{"cell_type":"markdown","metadata":{},"source":["Langchain 集成了超过 30 个不同的向量存储库。我们选择 Chroma 是因为它轻量级且数据存储在内存中，这使得它非常容易启动和开始使用。"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from langchain.vectorstores import Chroma\n","from langchain.document_loaders import PyMuPDFLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n","from zhipuai_embedding import ZhipuAIEmbeddings\n","\n","from langchain.llms import OpenAI\n","from langchain.llms import HuggingFacePipeline\n","from zhipuai_llm import ZhipuAILLM"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 使用前配置自己的 api 到环境变量中如\n","import os\n","import openai\n","import zhipuai\n","import sys\n","sys.path.append('../..')\n","\n","from dotenv import load_dotenv, find_dotenv\n","\n","_ = load_dotenv(find_dotenv()) # read local .env fileopenai.api_key  = os.environ['OPENAI_API_KEY']\n","openai.api_key  = os.environ['OPENAI_API_KEY']\n","zhipuai.api_key = os.environ['ZHIPUAI_API_KEY']"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# 加载 PDF\n","loaders_chinese = [\n","    PyMuPDFLoader(\"../../data_base/knowledge_db/LeeDL_Tutorial.pdf\") # 机器学习,\n","    # PyMuPDFLoader(\"../../docs/data_base/knowledge_db/easyRL.pdf\") # 强化学习,\n","    # PyMuPDFLoader(\"../../data_base/knowledge_db/LLM_Survey_Chinese.pdf\") # 大语言模型\n","]\n","docs = []\n","for loader in loaders_chinese:\n","    docs.extend(loader.load())\n","# 切分文档\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=150)\n","split_docs = text_splitter.split_documents(docs)\n","\n","\n","# 定义 Embeddings\n","# embedding = OpenAIEmbeddings() \n","# embedding = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n","embedding = ZhipuAIEmbeddings()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["persist_directory = '../../data_base/vector_db/chroma'"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["!rm -rf '../../data_base/vector_db/chroma'  # 删除旧的数据库文件（如果文件夹中有文件的话），window电脑请手动删除"]},{"cell_type":"markdown","metadata":{},"source":["## 二、构建 Chroma 向量库"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["vectordb = Chroma.from_documents(\n","    documents=split_docs[:100], # 为了速度，只选择了前 100 个切分的 doc 进行生成。\n","    embedding=embedding,\n","    persist_directory=persist_directory  # 允许我们将persist_directory目录保存到磁盘上\n",")"]},{"cell_type":"markdown","metadata":{},"source":["在此之后，我们要确保通过运行 vectordb.persist 来持久化向量数据库，以便我们在未来的课程中使用。\n","\n","让我们保存它，以便以后使用！"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["vectordb.persist()"]},{"cell_type":"markdown","metadata":{},"source":["大家也可以直接载入已经构建好的向量库"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["vectordb = Chroma(\n","    persist_directory=persist_directory,\n","    embedding_function=embedding\n",")"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["向量库中存储的数量：100\n"]}],"source":["print(f\"向量库中存储的数量：{vectordb._collection.count()}\")"]},{"cell_type":"markdown","metadata":{},"source":["## 三、通过向量数据库检索"]},{"cell_type":"markdown","metadata":{},"source":["### 3.1 相似度检索"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["question=\"什么是机器学习\""]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["检索到的内容数：3\n"]}],"source":["sim_docs = vectordb.similarity_search(question,k=3)\n","print(f\"检索到的内容数：{len(sim_docs)}\")"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["检索到的第0个内容: \n","→_→\n","https://github.com/datawhalechina/leedl-tutorial\n","←_←\n","第 1 章\n","机器学习基础\n","首先简单介绍一下机器学习（machine learning）和深度学习（deep learning）的基本概\n","念。机器学习，顾名思义，机器具备有学习的能力。具体来讲，机器学习就是让机器具备找一\n","个函数的能力。机器具备找函数的能力以后，它可以做很多事，举个例子：\n","--------------\n","检索到的第1个内容: \n","器不只是要做选择题或输出一个数字，而是产生一个有结构的物体，比如让机器画一张图，写\n","一篇文章。这种叫机器产生有结构的东西的问题称为结构化学习。\n","1.1\n","案例学习\n","以视频的点击次数预测为例介绍下机器学习的运作过程。假设有人想要通过视频平台赚\n","钱，他会在意频道有没有流量，这样他才会知道他的获利。假设后台可以看到很多相关的信\n","息，比如：每天点赞的人数、订阅人数、观看次数。根据一个频道过往所有的信息可以预\n","--------------\n","检索到的第2个内容: \n","前言\n","李宏毅老师是台湾大学的教授，其《机器学习》（2021 年春）是深度学习领域经典的中文视\n","频之一。李老师幽默风趣的授课风格深受大家喜爱，让晦涩难懂的深度学习理论变得轻松易懂，\n","他会通过很多动漫相关的有趣例子来讲解深度学习理论。李老师的课程内容很全面，覆盖了到深\n","度学习必须掌握的常见理论，能让学生对于深度学习的绝大多数领域都有一定了解，从而可以进\n","一步选择想要深入的方向进行学习，对于想入门深度学\n","--------------\n"]}],"source":["for i, sim_doc in enumerate(sim_docs):\n","    print(f\"检索到的第{i}个内容: \\n{sim_doc.page_content[:200]}\", end=\"\\n--------------\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["### 3.2 MMR 检索"]},{"cell_type":"markdown","metadata":{},"source":["如果只考虑检索出内容的相关性会导致内容过于单一，可能丢失重要信息。\n","\n","最大边际相关性 (`MMR, Maximum marginal relevance`) 可以帮助我们在保持相关性的同时，增加内容的丰富度。\n","\n","\n","核心思想是在已经选择了一个相关性高的文档之后，再选择一个与已选文档相关性较低但是信息丰富的文档。这样可以在保持相关性的同时，增加内容的多样性，避免过于单一的结果。"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["mmr_docs = vectordb.max_marginal_relevance_search(question,k=3)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["MMR 检索到的第0个内容: \n","→_→\n","https://github.com/datawhalechina/leedl-tutorial\n","←_←\n","第 1 章\n","机器学习基础\n","首先简单介绍一下机器学习（machine learning）和深度学习（deep learning）的基本概\n","念。机器学习，顾名思义，机器具备有学习的能力。具体来讲，机器学习就是让机器具备找一\n","个函数的能力。机器具备找函数的能力以后，它可以做很多事，举个例子：\n","--------------\n","MMR 检索到的第1个内容: \n","器不只是要做选择题或输出一个数字，而是产生一个有结构的物体，比如让机器画一张图，写\n","一篇文章。这种叫机器产生有结构的东西的问题称为结构化学习。\n","1.1\n","案例学习\n","以视频的点击次数预测为例介绍下机器学习的运作过程。假设有人想要通过视频平台赚\n","钱，他会在意频道有没有流量，这样他才会知道他的获利。假设后台可以看到很多相关的信\n","息，比如：每天点赞的人数、订阅人数、观看次数。根据一个频道过往所有的信息可以预\n","--------------\n","MMR 检索到的第2个内容: \n","15.4 元学习的应用 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279\n","第 16 章 终生学习\n","281\n","16.1 灾难性遗忘 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281\n","16.\n","--------------\n"]}],"source":["for i, sim_doc in enumerate(mmr_docs):\n","    print(f\"MMR 检索到的第{i}个内容: \\n{sim_doc.page_content[:200]}\", end=\"\\n--------------\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["可以看到内容有了更多的差异。"]},{"cell_type":"markdown","metadata":{},"source":["## 四、构造检索式问答连"]},{"cell_type":"markdown","metadata":{},"source":["我们已经可以通过向量数据库找到最相关的内容了，接下来我们可以让 LLM 来用这些相关的内容回答我们的问题。"]},{"cell_type":"markdown","metadata":{},"source":["### 4.1 直接询问 LLM"]},{"cell_type":"markdown","metadata":{},"source":["基于 LangChain，我们可以构造一个使用 LLM 进行问答的检索式问答链，这是一种通过检索步骤进行问答的方法。我们可以通过传入一个语言模型和一个向量数据库来创建它作为检索器。然后，我们可以用问题作为查询调用它，得到一个答案。"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# 导入检索式问答链\n","from langchain.chains import RetrievalQA"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["llm = OpenAI(temperature=0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 可以使用 HuggingFacePipeline 本地搭建大语言模型\n","model_id = 'THUDM/chatglm2-6b-int4' # 采用 int 量化后的模型可以节省硬盘占用以及实时量化所需的运算资源\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","model = AutoModel.from_pretrained(model_id, trust_remote_code=True).half().quantize(4).cuda()\n","model = model.eval()\n","pipe = pipeline(\n","    \"text2text-generation\",\n","    model=model, \n","    tokenizer=tokenizer, \n","    max_length=100\n",")\n","\n","llm = HuggingFacePipeline(pipeline=pipe)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["llm = ZhipuAILLM(model=\"chatglm_std\", temperature=0)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# 声明一个检索式问答链\n","qa_chain = RetrievalQA.from_chain_type(\n","    llm,\n","    retriever=vectordb.as_retriever()\n",")"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["大语言模型的回答为：\" 本知识库主要包含了机器学习和深度学习的相关知识，如基本概念、分类、回归、结构化学习等。通过详细的推导过程、重点讲解和强化，以及丰富的例子，使得深度学习的理论变得轻松易懂。此外，本知识库还涵盖了除公开课之外的其他深度学习相关知识，以帮助读者更好地理解和入门深度学习。\"\n"]}],"source":["# 可以以该方式进行检索问答\n","question = \"本知识库主要包含什么内容\"\n","result = qa_chain({\"query\": question})\n","print(f\"大语言模型的回答为：{result['result']}\")"]},{"cell_type":"markdown","metadata":{},"source":["### 4.2 结合 prompt 提问"]},{"cell_type":"markdown","metadata":{},"source":["对于 LLM 来说，prompt 可以让更好的发挥大模型的能力。\n","\n","\n","我们首先定义了一个提示模板。它包含一些关于如何使用下面的上下文片段的说明，然后有一个上下文变量的占位符。"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["from langchain.prompts import PromptTemplate\n","\n","# Build prompt\n","template = \"\"\"使用以下上下文片段来回答最后的问题。如果你不知道答案，只需说不知道，不要试图编造答案。答案最多使用三个句子。尽量简明扼要地回答。在回答的最后一定要说\"感谢您的提问！\"\n","{context}\n","问题：{question}\n","有用的回答：\"\"\"\n","QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# Run chain\n","qa_chain = RetrievalQA.from_chain_type(\n","    llm,\n","    retriever=vectordb.as_retriever(),\n","    return_source_documents=True,\n","    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",")"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["question = \" 2023 年大语言模型效果最好的是哪个模型\""]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["LLM 对问题的回答：\" 我无法回答这个问题，因为我的训练数据只到 2023，之后的信息和事件我无法了解。感谢您的提问！\"\n"]}],"source":["result = qa_chain({\"query\": question})\n","print(f\"LLM 对问题的回答：{result['result']}\")"]},{"cell_type":"markdown","metadata":{},"source":["这里因为没有对应的信息，所以大语言模型只能回答不知道。您可以将知识库的内容调整为大语言模型综述的内容重新进行尝试。"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["向量数据库检索到的最相关的文档：page_content='前言\\n李宏毅老师是台湾大学的教授，其《机器学习》（2021 年春）是深度学习领域经典的中文视\\n频之一。李老师幽默风趣的授课风格深受大家喜爱，让晦涩难懂的深度学习理论变得轻松易懂，\\n他会通过很多动漫相关的有趣例子来讲解深度学习理论。李老师的课程内容很全面，覆盖了到深\\n度学习必须掌握的常见理论，能让学生对于深度学习的绝大多数领域都有一定了解，从而可以进\\n一步选择想要深入的方向进行学习，对于想入门深度学习又想看中文讲解的同学是非常推荐的。\\n本教程主要内容源于《机器学习》（2021 年春），并在其基础上进行了一定的原创。比如，为\\n了尽可能地降低阅读门槛，笔者对这门公开课的精华内容进行选取并优化，对所涉及的公式都给\\n出详细的推导过程，对较难理解的知识点进行了重点讲解和强化，以方便读者较为轻松地入门。\\n此外，为了丰富内容，笔者在教程中选取了《机器学习》（2017 年春） 的部分内容，并补充了不\\n少除这门公开课之外的深度学习相关知识。\\n致谢\\n特别感谢 Sm1les、LSGOMYP 对本项目的帮助与支持。' metadata={'author': '', 'creationDate': \"D:20230831225119+08'00'\", 'creator': 'LaTeX with hyperref', 'file_path': '../../data_base/knowledge_db/LeeDL_Tutorial.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 1, 'producer': 'xdvipdfmx (20200315)', 'source': '../../data_base/knowledge_db/LeeDL_Tutorial.pdf', 'subject': '', 'title': '', 'total_pages': 330, 'trapped': ''}\n"]}],"source":["print(f\"向量数据库检索到的最相关的文档：{result['source_documents'][0]}\")"]},{"cell_type":"markdown","metadata":{},"source":["这种方法非常好，因为它只涉及对语言模型的一次调用。然而，它也有局限性，即如果文档太多，可能无法将它们全部适配到上下文窗口中。"]},{"cell_type":"markdown","metadata":{},"source":["langchain 提供了几种不同的处理文档的方法：\n","\n","|     类型      |                                定义/区别                                |                              优点                              |                              缺点                              |\n","|-------------|---------------------------------------------------------------------|----------------------------------------------------------------|----------------------------------------------------------------|\n","|   Stuff     | 将整个文本内容一次性输入给大模型进行处理。                               | - 只调用大模型一次，节省计算资源和时间。<br>- 上下文信息完整，有助于理解整体语义。<br>- 适用于处理较短的文本内容。 | - 不适用于处理较长的文本内容，可能导致模型过载。                |\n","|   Refine    | 通过多次调用大模型逐步改进文本质量，进行多次迭代优化。                          | - 可以在多次迭代中逐步改进文本质量。<br>- 适用于需要进行多次迭代优化的场景。 | - 增加了计算资源和时间的消耗。<br>- 可能需要多轮迭代才能达到期望的文本质量。<br>- 不适用于实时性要求较高的场景。 |\n","| Map reduce  | 将大模型应用于每个文档，并将输出作为新文档传递给另一个模型，最终得到单个输出。               | - 可以对多个文档进行并行处理，提高处理效率。<br>- 可以通过多次迭代处理实现优化。<br>- 适用于需要对多个文档进行处理和合并的场景。 | - 增加了计算资源和时间的消耗。<br>- 可能需要多轮迭代才能达到期望的结果。<br>- 不适用于处理单个文档的场景。 |\n","| Map re-rank | 在每个文档上运行初始提示，为答案给出一个分数，返回得分最高的响应。                        | - 可以根据置信度对文档进行排序和选择，提高结果的准确性。<br>- 可以提供更可靠的答案。<br>- 适用于需要根据置信度对文档进行排序和选择的场景。 | - 增加了计算资源和时间的消耗。<br>- 可能需要对多个文档进行评分和排序。<br>- 不适用于不需要对文档进行排序和选择的场景。 |"]},{"cell_type":"markdown","metadata":{},"source":["我们可以根据需要配置 chain_type 的参数，选择对应的处理方式。如：\n","```\n","RetrievalQA.from_chain_type(\n","    llm,\n","    retriever=vectordb.as_retriever(),\n","    chain_type=\"map_reduce\"\n",")\n","```"]}],"metadata":{"kernelspec":{"display_name":"Python 3.6.9 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":2}
