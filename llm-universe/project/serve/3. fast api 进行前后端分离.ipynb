{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7cf53b5",
   "metadata": {
    "height": 30
   },
   "source": [
    "# Fast api 进行前后端分离 💬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ce5955",
   "metadata": {
    "height": 30
   },
   "source": [
    "目前我们已经完成了基本的可视化页面，并且可以实现对应的功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d2d01b",
   "metadata": {},
   "source": [
    "为了方便整个项目的管理，现有的项目通常采用前后端分离的方式搭建，前后端数据通过 json 的格式进行传输。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e579ab22",
   "metadata": {},
   "source": [
    "FastAPI 是一个用于构建 API 的现代、快速（高性能）的 web 框架，非常方便用于搭建我们的前后端分离的应用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfda0137",
   "metadata": {},
   "source": [
    "我们首先需要将我们用到的后端函数进行 FastAPI 的封装。封装 API 与前文中讲过将大模型 API 封装成本地 API 的方法类似，我们首先导入第三方库并创建一个 API 对象："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e7d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import os\n",
    "\n",
    "app = FastAPI() # 创建 api 对象"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adc8f8e",
   "metadata": {},
   "source": [
    "本地 API 一般通过 POST 方式进行访问，即参数会附加在 POST 请求中，我们需要定义一个数据模型来接收 POST 请求中的数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495ff1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个数据模型，用于接收POST请求中的数据\n",
    "class Item(BaseModel):\n",
    "    prompt : str # 用户 prompt\n",
    "    model : str = \"gpt-3.5-turbo\"# 使用的模型\n",
    "    temperature : float = 0.1# 温度系数\n",
    "    if_history : bool = False # 是否使用历史对话功能\n",
    "    # API_Key\n",
    "    api_key: str = None\n",
    "    # Secret_Key\n",
    "    secret_key : str = None\n",
    "    # access_token\n",
    "    access_token: str = None\n",
    "    # APPID\n",
    "    appid : str = None\n",
    "    # APISecret\n",
    "    api_secret : str = None\n",
    "    # 数据库路径\n",
    "    db_path : str = \"../../data_base/vector_db/chroma\"\n",
    "    # 源文件路径\n",
    "    file_path : str = \"../../data_base/knowledge_db\"\n",
    "    # prompt template\n",
    "    prompt_template : str = template\n",
    "    # Template 变量\n",
    "    input_variables : list = [\"context\",\"question\"]\n",
    "    # Embdding\n",
    "    embedding : str = \"openai\"\n",
    "    # Top K\n",
    "    top_k : int = 5\n",
    "    # embedding_key\n",
    "    embedding_key : str = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a2fd1a",
   "metadata": {},
   "source": [
    "在上面的类中，我们定义了要调用我们已封装的 QA_chain 所需要传入的参数，对于非必须参数，我们都设置了默认参数来保证调用的简洁性，接下来我们就可以创建一个 POST 请求的 API 端点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ec91ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/answer/\")\n",
    "async def get_response(item: Item):\n",
    "\n",
    "    # 首先确定需要调用的链\n",
    "    if not item.if_history:\n",
    "        # 调用 Chat 链\n",
    "        chain = QA_chain_self(model=item.model, temperature=item.temperature, top_k=item.top_k, file_path=item.file_path, persist_path=item.db_path, \n",
    "                                appid=item.appid, api_key=item.api_key, embedding=item.embedding, template=template, api_secret=item.api_secret, embedding_key=item.embedding_key)\n",
    "\n",
    "        response = chain.answer(question = item.prompt)\n",
    "    \n",
    "        return response\n",
    "    \n",
    "    # 由于 API 存在即时性问题，不能支持历史链\n",
    "    else:\n",
    "        return \"API 不支持历史链\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e702e041",
   "metadata": {},
   "source": [
    "上述端点的业务逻辑很简单，即调用我们已封装的 QA_chain_self 对象进行实例化与回答即可。通过这一个端点启动，我们便可通过访问本地 8000 端口来调用个人知识库助手的服务啦，我们只需要通过下列命令启动： "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648f39a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "uvicorn app:app "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完整项目代码请查阅 [project/serve](/project/serve/) 目录~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
